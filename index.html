<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Sara Ghazanfari</title>

  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/jpg" href="Data/sara.JPG">
</head>


<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Sara Ghazanfari</name>
                  </p>
                  <p>
                    <div class="row" style="text-align: justify;">
            I am currently a Ph.D. candidate at New York University in the
            <a class="desc-links" href="https://wp.nyu.edu/ensure_group/">
                EnSuRe Research Group</a>.
            I'm pleased to be co-advised by <a class="desc-links" href="https://engineering.nyu.edu/faculty/siddharth-garg">
                Siddharth Garg</a> and <a class="desc-links" href="https://engineering.nyu.edu/faculty/farshad-khorrami">
                Farshad Khorrami</a>. My current research is focused on <b>building robust and scalable multimodal
                  perception systems</b> that can operate in the real world. My latest work,
                  <a class="desc-links" href="https://arxiv.org/abs/2410.02080"><b>EMMA</b></a>, proposes an efficient
                  modality adaptation module to provide <b>instruction-aware visual representations to MLLM</b>
                  (Multi-modal Large Language Models). My earlier work
                  <a class="desc-links" href="https://arxiv.org/abs/2410.02080"><b>LipSim</b></a>
                  introduces a <b>robust perceptual metric</b>
                  by leveraging Lipschitz Model as backbone and distilling from the SOTA metric
                  <a href="https://arxiv.org/abs/2306.09344">DreamSim</a>.
                  </div>
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:sg7457@nyu.edu">Email</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=0dMW47QAAAAJ&hl=en">Google Scholar</a>
                    &nbsp/&nbsp
                    <a href="https://github.com/SaraGhazanfari">GitHub</a> &nbsp/&nbsp
                    <a href="https://twitter.com/SaraGhznfri">Twitter</a> &nbsp/&nbsp
                    <a href="https://www.linkedin.com/in/sara-ghazanfari-1a8b37163/">LinkedIn</a> &nbsp/&nbsp
                    <a href="Data/my_cv.pdf">CV</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="Data/sara.JPG"><img style="width:100%;max-width:100%" alt="profile photo"
                      src="Data/sara.JPG" class="hoverZoomLink"></a>
                </td>
              </tr>
          </table>




          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tr>
              <td width="100%" valign="middle">
                <heading>News</heading>
                <!-- <br /> -->
                <ul>
                  <li><b>Jan-2024:</b> One paper accepted to ICLR 2024. </li>
                </ul>
                <ul>
                  <li><b>June-2023:</b> One paper accepted to ICML Workshop 2023. </li>
                </ul>
              </td>
            </tr>
          </table>





          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Research</heading>
<!--                  <p>-->
<!--                    -->
<!--                  </p>-->
                </td>
              </tr>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
                            <tr onmouseout="bob_stop()" onmouseover="bob_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='bob_image'>
                      <img src='Data/emma.png' width="180">
                    </div>
                    <img src='Data/emma.png' width="180">
                  </div>
                  <script type="text/javascript">
                    function bob_start() {
                      document.getElementById('bob_image').style.opacity = "0";
                    }

                    function bob_stop() {
                      document.getElementById('bob_image').style.opacity = "1";
                    }
                    bob_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2410.02080">
                    <papertitle> EMMA: Efficient Visual Alignment in Multi-Modal LLMs
                    </papertitle>
                  </a>
                  <br>
                  <strong>S. Ghazanfari</strong>, A. Araujo, P. Krishnamurthy, S. Garg and F. Khorrami
                  <br>
                  <font color="red"><strong><em>Submitted to ICLR</em>, 2025</strong></font>
                  <br>
                  <a href="https://arxiv.org/abs/2410.02080.pdf">PDF</a> /
                  <a href="https://arxiv.org/abs/2410.02080">arXiv</a> /
                  <a href="https://github.com/SaraGhazanfari/EMMA">code</a>
                  <br>
                  <p></p>
                  <p>
                     <div class="row" style="text-align: justify;">
                  In this paper, we propose EMMA (Efficient Multi-Modal Adaptation), a lightweight cross-modality
                  module designed to efficiently fuse visual and textual encodings, generating <b>instruction-aware
                  visual representations for the language model</b>. Our key contributions include: (1) an efficient
                  early fusion mechanism that integrates vision and language representations with minimal added
                  parameters (less than 0.2% increase in model size), (2) an in-depth interpretability analysis
                  that sheds light on the internal mechanisms of the proposed method; (3) comprehensive experiments
                  that demonstrate notable improvements on both specialized and general benchmarks for MLLMs.
                  Empirical results show that EMMA boosts performance across multiple tasks by up to 9.3% while
                  significantly improving robustness against hallucinations.

                </div>
                  </p>
                </td>
              </tr>

              <tr onmouseout="bob_stop()" onmouseover="bob_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='bob_image'>
                      <img src='Data/lipsim_ex.png' width="180">
                    </div>
                    <img src='Data/lipsim_ex.png' width="180">
                  </div>
                  <script type="text/javascript">
                    function bob_start() {
                      document.getElementById('bob_image').style.opacity = "0";
                    }

                    function bob_stop() {
                      document.getElementById('bob_image').style.opacity = "1";
                    }
                    bob_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2310.18274">
                    <papertitle> LipSim: A Provably Robust Perceptual Similarity Metric
                    </papertitle>
                  </a>
                  <br>
                  <strong>S. Ghazanfari</strong>, A. Araujo, P. Krishnamurthy, F. Khorrami and S. Garg
                  <br>
                  <font color="red"><strong><em>ICLR</em>, 2024</strong></font>
                  <br>
                  <a href="https://arxiv.org/pdf/2310.18274.pdf">PDF</a> /
                  <a href="https://arxiv.org/abs/2310.18274">arXiv</a> /
                  <a href="https://github.com/SaraGhazanfari/lipsim">code</a>
                  <br>
                  <p></p>
                  <p>
                     <div class="row" style="text-align: justify;">
                  In this work, we demonstrate the vulnerability of the SOTA perceptual similarity metric
                  based on an ensemble of ViT-based feature extractors to adversarial attacks.
                  We then propose a framework to train a robust perceptual similarity metric called LipSim
                  (Lipschitz Similarity Metric) with provable guarantees by leveraging 1-Lipschitz neural
                  networks as backbone and knowledge distillation approach to distill the knowledge of the
                  SOTA models. Finally, a comprehensive set of experiments shows the performance of LipSim
                  in terms of natural and certified scores and on the image retrieval application.

                </div>
                  </p>
                </td>
              </tr>

               <tr onmouseout="bob_stop()" onmouseover="bob_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='bob_image'>
                      <img src='Data/r_lpips.png' width="180">
                    </div>
                    <img src='Data/r_lpips.png' width="180">
                  </div>
                  <script type="text/javascript">
                    function bob_start() {
                      document.getElementById('bob_image').style.opacity = "0";
                    }

                    function bob_stop() {
                      document.getElementById('bob_image').style.opacity = "1";
                    }
                    bob_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2307.15157">
                    <papertitle> R-LPIPS: An Adversarially Robust Perceptual Similarity Metric
                    </papertitle>
                  </a>
                  <br>
                  <strong>S. Ghazanfari</strong>, S. Garg, P. Krishnamurthy, F. Khorrami and A. Araujo
                  <br>
                  <font color="red"><strong><em>ICML Workshop</em>, 2023</strong></font>
                  <br>
                  <a href="https://arxiv.org/pdf/2307.15157.pdf">PDF</a> /
                  <a href="https://arxiv.org/abs/2307.15157">arXiv</a> /
                  <a href="https://github.com/SaraGhazanfari/r-lpips">code</a>
                  <br>
                  <p></p>
                  <p>
                     <div class="row" style="text-align: justify;">
                  In this work, we show that the LPIPS metric is sensitive to adversarial perturbation and propose the
                  use of Adversarial Training to build a new Robust Learned Perceptual Image Patch Similarity (R-LPIPS)
                  that leverages adversarially trained deep features. Based on an adversarial evaluation, we demonstrate
                  the robustness of R-LPIPS to adversarial examples compared to the LPIPS metric.
                  Finally, we showed that the perceptual defense achieved over LPIPS metrics could easily
                  be broken by stronger attacks developed based on R-LPIPS.
                </div>
                  </p>
                </td>
              </tr>

        </td>
      </tr>
  </table>
</body>

</html>